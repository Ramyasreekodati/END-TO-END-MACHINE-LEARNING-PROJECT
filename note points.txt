# what is mean by setup.pu ?
It is a blueprint .It reduces the error.It uses tools like "requriments.txt" or "pyproject.toml" allowa consistent installation of dependencies.

Folders: Organize your workbooks
         (e.g., data, code, tests).
Files: Keep important notes 
        (e.g., configuration, scripts).
Tools: Have your pens ready 
        (e.g., software, libraries, virtual environments).



##Day -1 :
1.project Introduction
2. Github Repo setup
3. Project Template
4. requriments Installation (python Packages)
 
#Tools Installed:

Conda: For environment management.
Git Bash: For version control operations.
Rustinit: To handle dependencies required for some Python libraries.
Pywinpty: To ensure compatibility with Windows environments.
Process Followed:

#Environment and Tool Integration

Configured the environment path.
Linked Conda and Git Bash with Visual Studio Code.
#Repository Management

Created a new GitHub repository.
Created a local project folder on the system.

#Cloned the repository using the following steps:
    git clone <repository_url>
    git init
    git add .
    git commit -m "Initial commit"
    git branch
    git push origin main


## Day 2:
1. Notebook Experiment
2. Project utils --> Logging, exceptions, utils module,
3. project workflows


# what is mean by Logging?
# what is mean by exceptions
# what is mean by utils module
# what is mean by workflows , why we should need it .
# how to log transform sklearn
# how to reduse skuwness
# how to create pr problem
# elastic net sklearn


###Logging
Logging is a method to track events in your application during execution.

Purpose: Helps monitor, debug, and maintain the codebase by storing important information about execution.

How It Works:

Configured using the logging module in Python.
Logs can include timestamps, log levels (INFO, DEBUG, ERROR), the module name, and custom messages.

Example log format:
ruby

                [%(asctime)s: %(levelname)s: %(module)s: %(message)s]

Example output:
yaml
                [2024-11-24 05:43:44,918: INFO: main: This is my logger]
Can redirect logs to files or console for better tracking.
Logs are configured in an __init__.py file or main module.

Example:
python

                from src.wine_quality.logging import logger
                logger.info("This is a test log")
Benefits:

>>>Tracks the execution process with clear timestamps and messages.
>>>Reduces redundancy by centralizing log operations.
>>>Simplifies debugging.


###YAML Files

YAML (Yet Another Markup Language) is used to define configurations in a readable format.

Usage in Python:
The pyyaml package is used to read and parse YAML files.
Simplifies configuration handling in projects.

Why YAML for Logging?
Keeps logging and configurations organized and avoids repetitive code.

###Config Module

A configuration module simplifies extracting values.
Instead of using traditional dictionary indexing (dict["key"]), configurations can be accessed as attributes (dict.key).
This approach reduces errors and makes the code cleaner.
  
Why Use the ensure_annotation Decorator?

Ensures that the functions return expected outputs by validating inputs and outputs.
Helps maintain consistency in utility code.


###Exceptions

Exceptions handle runtime errors gracefully, ensuring the program doesnot crash unexpectedly.

Why Use Exceptions?

Provides clear error messages.
Helps isolate and fix issues without breaking the entire workflow.
Custom Exceptions:

User-defined exceptions tailored to specific error cases in your application.
Example: Using BoxException to capture and log specific errors.

### workflow
A workflow is a sequence of steps or processes designed to complete a specific task in a project.

Purpose: Provides structure to complex tasks, ensuring consistency and efficiency.
Why It’s Needed:
Defines a clear sequence for project execution.
Helps teams collaborate effectively.
Automates repetitive tasks.
Example:
In a machine learning project, workflows might include:
Data collection → Data cleaning → Feature engineering → Model training → Evaluation.


###Reduce Skewness?
Skewness can be reduced by applying transformations to the data:

Techniques:
Log Transformation: np.log1p(data)
Square Root Transformation: np.sqrt(data)
Box-Cox Transformation (only for positive data):
from scipy.stats import boxcox
transformed_data, _ = boxcox(data)


###Day 3 :
1. Data ingestion component -->github --> Notebook
2. Data validating
3. Data Transformation



config ---->>> config.yaml  
* Data Ingestion  
* Data Validation  
* Data Transformation  
* Data Trainer  
* Data Evaluation  

                >>>> Data Ingestion >>>>  
                        artifacts  
                            |  
                      data ingestion  

* Loading the raw data from the source  
* Unzipping the data and organizing it into a structured format  
* Storing the ingested data for further processing  

                >>>> Data Validation >>>>  
                        artifacts  
                            |  
                      data validation  

* Ensuring data quality by checking for missing, inconsistent, or duplicate values  
* Validating schema, file structure, and data types  
* Generating reports or logs for validation results  

                >>>> Data Transformation >>>>  
                        artifacts  
                            |  
                      data transformation  

* Cleaning the data by handling missing values, outliers, or irrelevant data  
* Encoding categorical variables, normalizing/standardizing data  
* Creating new features or engineering additional insights  

                >>>> Data Trainer >>>>  
                        artifacts  
                            |  
                      data trainer  

* Splitting the dataset into training and testing datasets  
* Selecting and training the appropriate machine learning models  
* Hyperparameter tuning to optimize model performance  

                >>>> Data Evaluation >>>>  
                        artifacts  
                            |  
                      data evaluation  

* Evaluating the trained model using metrics such as accuracy, precision, recall, F1-score, etc.  
* Generating performance reports and comparing models  
* Finalizing the best-performing model for deployment or further use  



├── research  
│       ├── 01_data_ingestion.py  
├── config  
│       └── config.yaml        
├── src  
│   ├── wine_quality  
│   │   ├── utils  
│   │   │   └── common.py  (Handles YAML reading and directory creation)  
│   │   ├── entity  
│   │   │   └── config_entity.py  
│   │   ├── component  
│   │   │   ├── data_ingestion.py  
│   │   │   
│   │   ├── pipeline  
│   │   │   ├── stage_01_data_ingestion.py  
│   │   │    
│   │   ├── constant  
│   │   │   └── constants.py  
│   │   └── __init__.py  
├── params.yaml  (Parameter configurations)  
└── schema.yaml  (Data types and schema validation)  


# 1. Modular Coding Overview
All modules interconnect via pipeline stages.
Key modular components:
mathematica
Copy code
research ----> 01.data_ingestion
   wine_quality
      ├── entity.config_entity  
      ├── config.configuration  
      ├── component.data_ingestion 
      └── pipeline.stage_01_data_ingestion 


# Data Classes in Python
Introduced in Python 3.7 via the dataclasses module.
Simplifies creation and management of classes used for data storage.
By using @dataclass, you avoid boilerplate code like:
__init__
__repr__
__eq__



## Day 4
1. Model Trainer component
2. Model Evaluation component
3. Prediction pipeline
4. User APP ---> Flask



joblib or we can use pickle
